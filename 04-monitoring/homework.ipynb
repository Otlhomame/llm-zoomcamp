{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = \"https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv\"\n",
    "\n",
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the data\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the column of the dataset\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"multi-qa-mpnet-base-dot-v1\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_llm = df_new.iloc[0].answer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.encode(answer_llm)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to display progress bar for loops and iterators\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = []\n",
    "\n",
    "for index, record in tqdm(df_new.iterrows()):\n",
    "    answer_org = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    llm = embedding_model.encode(answer_org)\n",
    "    orig = embedding_model.encode(answer_llm)\n",
    "    \n",
    "    dot_product = llm.dot(orig)\n",
    "    evaluation.append(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"score\"] = evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependency\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(df, embedding_model):\n",
    "    similarity = []\n",
    "\n",
    "    for index, record in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        answer_org = record['answer_orig']\n",
    "        answer_llm = record['answer_llm']\n",
    "        \n",
    "        # Encode the answers to get the vectors\n",
    "        llm = embedding_model.encode(answer_llm)\n",
    "        orig = embedding_model.encode(answer_org)\n",
    "        \n",
    "        # Compute norms for each vector\n",
    "        norm_llm = np.sqrt(np.sum(llm ** 2))\n",
    "        norm_orig = np.sqrt(np.sum(orig ** 2))\n",
    "        \n",
    "        # Normalize the vectors\n",
    "        llm_norm = llm / norm_llm\n",
    "        orig_norm = orig / norm_orig\n",
    "        \n",
    "        # Compute the cosine similarity (dot product of normalized vectors)\n",
    "        dot_product = np.dot(llm_norm, orig_norm)\n",
    "        similarity.append(dot_product)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"cosine\"] = cosine_similarity(df_new, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"cosine\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = df_new.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['rouge-1']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1_f1 = scores['rouge-1']['f']\n",
    "rouge_2_f1 = scores['rouge-2']['f']\n",
    "rouge_l_f1 = scores['rouge-l']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_f1 = (rouge_1_f1 + rouge_2_f1 + rouge_l_f1) / 3\n",
    "average_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_2_f1_scores = []\n",
    "\n",
    "# Compute ROUGE-2 F-scores for all records\n",
    "for index, record in tqdm(df_new.iterrows(), total=df.shape[0]):\n",
    "    scores = rouge_scorer.get_scores(record['answer_llm'], record['answer_orig'])[0]\n",
    "    rouge_2_f1 = scores['rouge-2']['f']\n",
    "    rouge_2_f1_scores.append(rouge_2_f1)\n",
    "\n",
    "# Compute the average ROUGE-2 F-score\n",
    "average_rouge_2_f1 = sum(rouge_2_f1_scores) / len(rouge_2_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average ROUGE-2 F1 Score across all records: {average_rouge_2_f1}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
